{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "R5LWOJgtJx8U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sympy as sp\n",
        "import re\n",
        "\n",
        "def load_file(file_path: str) -> pd.DataFrame:\n",
        "    data = open(file_path, \"r\").readlines()\n",
        "    data = [line.strip().split(\"=\") for line in data]\n",
        "\n",
        "    # Extract function, variable, and derivative from each line\n",
        "    rows = [(line[0].split('d(')[-1].split(')/d')[0], line[0].split(')/d')[1], line[1]) for line in data]\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(rows, columns=['Function', 'Variable', 'Derivative'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def tokenize(expression):\n",
        "    # Use regular expressions to split the expression into tokens\n",
        "    tokens = re.findall(r'\\d+\\.?\\d*|\\w+|[+\\-*/()^]', expression)\n",
        "    return tokens\n",
        "\n",
        "def tokenize_dataframe(df):\n",
        "    # Create a set of all unique tokens in the DataFrame\n",
        "    all_tokens = set()\n",
        "\n",
        "    for col in ['Function', 'Variable', 'Derivative']:\n",
        "        all_tokens.update(df[col].apply(tokenize).explode().unique())\n",
        "\n",
        "    # Define a dictionary to map tokens to unique numerical values\n",
        "    token_to_index = {token: i for i, token in enumerate(all_tokens)}\n",
        "    index_to_token = {i: token for token, i in token_to_index.items()}\n",
        "\n",
        "    # Convert tokens to numerical values\n",
        "    for col in ['Function', 'Variable', 'Derivative']:\n",
        "        df[f'{col}_indices'] = df[col].apply(lambda expr: [token_to_index[token] for token in tokenize(expr)])\n",
        "\n",
        "    return df, token_to_index, index_to_token\n",
        "\n",
        "# Example usage\n",
        "filepath = \"train.txt\"\n",
        "df = load_file(filepath)\n",
        "\n",
        "# Tokenize expressions, variables, and derivatives in the DataFrame\n",
        "df, token_to_index, index_to_token = tokenize_dataframe(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dYeZ_4CyP6sj"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['Function', 'Variable', 'Derivative'], axis = 1)\n",
        "\n",
        "# Assuming df is your DataFrame with the original column names\n",
        "df.rename(columns={'Function_indices': 'Function', 'Variable_indices': 'Variable', 'Derivative_indices': 'Derivative'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOPBplBPP78g",
        "outputId": "a6f02fe4-b7c7-4805-829e-4b7c67ae355d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 219s 9ms/step - loss: 0.5671 - accuracy: 0.8511\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 213s 9ms/step - loss: 0.1119 - accuracy: 0.9723\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 216s 9ms/step - loss: 0.0487 - accuracy: 0.9883\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 215s 9ms/step - loss: 0.0301 - accuracy: 0.9926\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 217s 9ms/step - loss: 0.0223 - accuracy: 0.9944\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 220s 9ms/step - loss: 0.0182 - accuracy: 0.9953\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 222s 9ms/step - loss: 0.0158 - accuracy: 0.9958\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 219s 9ms/step - loss: 0.0142 - accuracy: 0.9961\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 220s 9ms/step - loss: 0.0130 - accuracy: 0.9964\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 221s 9ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "6250/6250 [==============================] - 35s 6ms/step - loss: 0.0129 - accuracy: 0.9964\n",
            "Validation Loss: 0.01287948526442051, Validation Accuracy: 0.996431827545166\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Masking\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming num_tokens is the total number of unique tokens\n",
        "num_tokens = len(index_to_token)\n",
        "max_sequence_length = 30  # Maximum sequence length\n",
        "\n",
        "# Assuming df is your DataFrame with columns 'Function', 'Variable', and 'Derivative'\n",
        "function_indices = np.array(df['Function'].tolist(), dtype=object)\n",
        "variable_indices = np.array(df['Variable'].tolist(), dtype=object)\n",
        "derivative_indices = np.array(df['Derivative'].tolist(), dtype=object)\n",
        "\n",
        "# Ensure sequences are of integer type\n",
        "function_indices = [np.array(seq, dtype=int) for seq in function_indices]\n",
        "variable_indices = [np.array(seq, dtype=int) for seq in variable_indices]\n",
        "derivative_indices = [np.array(seq, dtype=int) for seq in derivative_indices]\n",
        "\n",
        "# Pad or truncate the sequences to the specified length\n",
        "function_indices = tf.keras.preprocessing.sequence.pad_sequences(function_indices, maxlen=max_sequence_length, padding='post')\n",
        "variable_indices = tf.keras.preprocessing.sequence.pad_sequences(variable_indices, maxlen=max_sequence_length, padding='post')\n",
        "derivative_indices = tf.keras.preprocessing.sequence.pad_sequences(derivative_indices, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Concatenate the sequences\n",
        "target_indices = np.concatenate([np.zeros_like(derivative_indices[:, :1]), derivative_indices[:, :-1]], axis=1)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "function_indices_train, function_indices_val, variable_indices_train, variable_indices_val, target_indices_train, target_indices_val = train_test_split(\n",
        "    function_indices, variable_indices, target_indices, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define model architecture\n",
        "embedding_dim = 50\n",
        "latent_dim = 100\n",
        "\n",
        "input_function = Input(shape=(max_sequence_length,))\n",
        "input_variable = Input(shape=(max_sequence_length,))\n",
        "encoder_embedding = Embedding(num_tokens, embedding_dim, input_length=max_sequence_length)\n",
        "\n",
        "function_embedding = encoder_embedding(input_function)\n",
        "variable_embedding = encoder_embedding(input_variable)\n",
        "\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "_, function_state_h, function_state_c = encoder_lstm(function_embedding)\n",
        "_, variable_state_h, variable_state_c = encoder_lstm(variable_embedding)\n",
        "\n",
        "encoder_states = [function_state_h, function_state_c, variable_state_h, variable_state_c]\n",
        "\n",
        "input_derivative = Input(shape=(max_sequence_length,))\n",
        "decoder_embedding = Embedding(num_tokens, embedding_dim, input_length=max_sequence_length)\n",
        "\n",
        "derivative_embedding = decoder_embedding(input_derivative)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(derivative_embedding, initial_state=encoder_states[:2])\n",
        "\n",
        "decoder_dense = Dense(num_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([input_function, input_variable, input_derivative], decoder_outputs)\n",
        "\n",
        "# Compile the model with accuracy as a metric\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit([function_indices_train, variable_indices_train, target_indices_train], derivative_indices_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "loss, accuracy = model.evaluate([function_indices_val, variable_indices_val, target_indices_val], derivative_indices_val)\n",
        "\n",
        "print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RnxNKF2ZyyMe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}